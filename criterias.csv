Pitfall,Question Code,CATEGORY,Criterion Question,Response Type,Scale,No,Yes,1,2,3,4,5,How to Check / Evidence to Extract,Notes
Base Rate Fallacy,P1 -Q1,Detection Rate Context,"When reporting high TPR (“we detect 98%”), do they contextualize with FPR/base rate?",Yes/No,1/0,TPR alone without context / No contextualisation,TPR paired with realistic FPR/base-rate or precision,N/A,N/A,N/A,N/A,N/A,Text near TPR claims; look for base-rate mention,
Base Rate Fallacy,P1 -Q2,Base-Rate Discussion,Do they discuss the real base rate (prevalence) and the base-rate fallacy risk?,Ordinal,1–5,N/A,N/A,No mention of base rates or the base-rate fallacy ,"Brief or vague mention that prevalence matters, but no explanation or connection to precision or real-world interpretation.","Acknowledges that real-world base rates affect model performance and mentions the base-rate fallacy, but with limited detail or examples.","Clearly discusses how prevalence influences posterior precision or predictive value, and explicitly warns about base-rate fallacy with at least one clear example or implication.","Provides a detailed explanation of base-rate impact using quantitative reasoning (e.g., Bayes’ theorem or confusion matrix terms) / Directly illustrates the base-rate fallacy risk and connects it to real-world decision-making.",Background/results sections,
Base Rate Fallacy,P1 -Q3,Confusion Matrix,Do they provide a confusion matrix or per-class TP/FP/TN/FN (and derived rates)?,Yes/No,1/0,None provided,Confusion matrix or equivalent per-class stats present,N/A,N/A,N/A,N/A,N/A,Methods/results appendix or main text,
Base Rate Fallacy,P1 -Q4,Degree of Class Imbalance,How significant is the class imbalance reported in their work?,Ordinal,1-5,N/A,N/A,Larger class > 90,80<Larger class < 89,70 <Larger class < 79,60 <Larger class < 69,49<Larger class < 59,Dataset section ,
Base Rate Fallacy,P1 -Q5,Erroneous conclusion,Do they avoid drawing conclusions using inappropriate measures?,Yes/No,1/0,Use of inappropriate measures,No inappropriate measures used,N/A,N/A,N/A,N/A,0,Results section,"Inappropriate metrics: Accuracy, Recall (when used as the ONLY metric)"
Base Rate Fallacy,P1 -Q6,Erroneous conclusion,Which inappropriate measures are they using to draw conclusions?,Categorical,N/A,Includes but not limited to: ,"Accuracy, Recall (When used alone), Error rate",N/A,N/A,N/A,N/A,N/A,Results section,
Base Rate Fallacy,P1 -Q7,Precision,What is their precision?,Numeric,0-1,N/A,N/A,N/A,N/A,N/A,N/A,N/A,"Results section; Put an N/A if they do not provide precision or the TP and FP, needed to calculate it","What if multiple datasets are used??? >Separate the precisions with commas (ex: 0.3, 0.5)"
Base Rate Fallacy,P1 -Q8,Precision,How high is their precision?,Ordinal,1-5,N/A,N/A,0-10%,11-50%,51-75 %,56-94%,95-100 %,"Results section; Put an N/A if they do not provide precision or the TP and FP, needed to calculate it",
Base Rate Fallacy,P1 -Q9,Mitigation,Do they use appropriate metrics or methods to draw conclusions?,Yes/No,1/0,No appropriate measures used,Use of appropriate measures,N/A,N/A,N/A,N/A,N/A,Results section,"Appropriate metrics or methods: Precision, Recall (Paired with other metrics such as precision), ROC/AUC curves, Matthews Correlation Coefficient  (MMC), F1 score,  Bayesian Posterior Probability..."
Base Rate Fallacy,P1 -Q10,Mitigation,Which appropriate metrics or methods are they using to draw conclusions?,Categorical,N/A,Includes but not limited to: ,"Precision, F1 score, MMC, Bayesian Posterior Probability",N/A,N/A,N/A,N/A,N/A,Results section,
Inappropriate Baseline,P2 -Q1,Baseline Relevance,"Does the chosen baseline represent a current and competitive IDS model, rather than an outdated or trivial one (e.g., Naïve Bayes, Decision Tree)?",Ordinal,1–5,N/A,N/A,Outdated or trivial baseline,"Slightly outdated, limited comparison","Reasonably recent, partially justified",Recent and justified baseline,Reflects state-of-the-art with evidence,Look for model names and publication years in Methods or Related Work,
Inappropriate Baseline,P2 -Q2,Baseline Variety,Does the study compare the proposed model to multiple baselines (simple + advanced)?,Ordinal,1–5,N/A,N/A,Only one weak comparator,Two weak comparators,Two average comparators,Multiple but unbalanced,Several strong baselines covering different IDS types,Results tables; “Comparative Analysis” section,
Inappropriate Baseline,P2 -Q3,Baseline Justification,Do the authors justify why each baseline is relevant to their study or dataset?,Ordinal,1–5,N/A,N/A,No explanation,Minimal mention,Generic justification,Linked to task partially,Detailed justification aligned with context,Methods or Related Work,
Inappropriate Baseline,P2 -Q4,Benchmark & Dataset Use,"Does the study make use of established benchmark datasets (such as CICIDS2017, UNSW-NB15, and NSL-KDD) for evaluating IDS?
 (Newer datasets, such as CICIDS2017 or CSE-CIC-IDS2018, are regarded as current; older datasets, such as KDD99 or NSL-KDD, are regarded as out-of-date.)",Ordinal,1–5,N/A,N/A,"Private, unclear, or self-collected dataset","Old or niche dataset (e.g., KDD99, small-scale)",Common dataset but not justified,"Public benchmark partially described (e.g., CICIDS2017)","Recognized benchmark dataset (e.g., CICIDS2017, UNSW-NB15, CSE-CIC-IDS2018) clearly justified and referenced",Dataset section; data citations,"I would include here examples of old dfatasets, new ones so that everyone can classify them the same way\"
Inappropriate Baseline,P2 -Q5,Dataset & Metric Consistency,"Do the authors evaluate both the baseline and the proposed model using the same dataset partitions (e.g., identical train/test splits), preprocessing steps, and performance metrics?",Ordinal,1–5,N/A,N/A,Different datasets or inconsistent evaluation metrics are used between the baseline and the proposed model.,"The same dataset is used, but different train/test splits or evaluation setups are applied.","The same dataset and metrics are mentioned, but split consistency is unclear or not documented.","The same dataset, metrics, and preprocessing are mostly consistent, with minor variations between models.","Fully identical dataset partitions (train/test splits), preprocessing steps, and evaluation metrics are used and clearly described.",Methods / Evaluation subsection,"you will be looking for ""do they use the same data set"" instead you will be looking for ""do they use the same splits to evaluate the algorithms"""
Inappropriate Baseline,P2 -Q6,Fair Optimization Effor,"Is equal tuning effort (e.g., hyperparameters, feature selection) applied to both baseline and model?",Ordinal,1–5,N/A,N/A,Only proposed model tuned,Baseline tuned slightly,Both tuned but unevenly,Similar tuning effort,"Equal, clearly documented tuning",Implementation section or Appendix,
Inappropriate Baseline,P2 -Q7,Performance Gap Plausibility,Are the reported improvements over baselines reasonable (<30%) and justified?,Ordinal,1–5,N/A,N/A,Unexplained large gap,Slightly inflated,Moderately justified,Realistic with data support,Well-justified and consistent improvements,Results and Discussion,
Inappropriate Baseline,P2 -Q8,Baseline Implementation Integrity,Is the baseline properly implemented and verified (not broken or simplified)?,Ordinal,1–5,N/A,N/A,Misconfigured / unclear implementation,Weak but functional,Adequate implementation,Valid baseline partially verified,Verified and consistent with literature,Appendix / Code repository,
Inappropriate Baseline,P2 -Q9,Transparency of Baseline Reporting,"Are baseline details (architecture, parameters, datasets) clearly described for replication?",Ordinal,1–5,N/A,N/A,Missing or vague,Minimal description,Partial but insufficient,Almost complete,Fully transparent and replicable,Methods / Supplementary material,
Inappropriate Baseline,P2 -Q10,Baseline Source Credibility,"Are baselines taken from peer-reviewed or reliable open sources, not self-defined ones?",Ordinal,1–5,N/A,N/A,Custom weak baseline,Uncited or unclear source,Partially cited,Referenced but old,Peer-reviewed and credible sources,References / Methods section,
Inappropriate Baseline,P2 -Q11,Operational Relevance,"Are the baselines aligned with real-world IDS types (e.g., comparing anomaly- vs. signature-based)?",Ordinal,1–5,N/A,N/A,No relation to real IDS use,Minimal mention,Theoretical alignment,Some operational connection,Realistic operational baseline,Discussion / Related work,
Inappropriate Baseline,P2 -Q12,Reproducibility & Accessibility,Are baseline and model implementations / data available for replication?,Yes/No,1/0,No code or data,Public repository or full details,_,_,_,_,_,Appendix / GitHub link,
Inappropriate Baseline,P2 -Q13,Result Comparison Fairness,"Are results for baseline and model obtained under identical conditions (same seed, metrics, cross-validation)?",Ordinal,1–5,N/A,N/A,Unclear / inconsistent,Partially different,Mostly similar,Nearly identical,Fully identical and verified,Experiment setup section,
Inappropriate Baseline,P2 -Q14,Baseline Limitations Discussion,Do authors acknowledge baseline weaknesses and their impact on the results?,Ordinal,1–5,N/A,N/A,None mentioned,Minimal comment,Some mention,Balanced discussion,Clear acknowledgment and justification,Discussion section,
Inappropriate Measures,P3 -Q1,Class Imbalance Awareness,"Do chosen metrics acknowledge imbalance (e.g., Precision/Recall, PR-AUC, MCC) instead of only Accuracy/AUC?",Ordinal,1–5,N/A,N/A,1 = Accuracy or plain AUC is primary; PR ignored,2 = Brief mention of imbalance but no PR/PR-AUC/MCC numbers reported,"3 = PR/PR-AUC (or precision/recall) reported at least once (e.g., in appendix or a single table) but not emphasized","4 = PR/PR-AUC (and/or precision/recall) reported in main results and compared across methods, with thresholds/operating points; accuracy/AUC still appears but not primary",5 = Uses PR/PR-AUC + Precision/Recall prominently (and maybe MCC),"Metrics section, main tables/figures",
Inappropriate Measures,P3 -Q2,Accuracy Misuse,Is Accuracy (or error rate) avoided as a primary success metric on imbalanced data?,Ordinal,1–5,N/A,N/A,1 = Accuracy is headline metric with no caveat,2 = Accuracy is headline but a brief note mentions imbalance without alternatives,3 = Accuracy appears but PR metrics (precision/recall or PR-AUC) are reported somewhere (not emphasized),4 = Accuracy present but clearly secondary; PR metrics are primary in figures/tables,5 = Accuracy is deemphasized or contextualized as misleading under imbalance,"Abstract, results summary; any “>95% accuracy” claims without caveats",
Inappropriate Measures,P3 -Q3,ROC/AUC Context,"Are ROC/AUC results complemented by low-FPR views (e.g., bounded AUC ≤0.1% FPR) or reported operating points?",Ordinal,1–5,N/A,N/A,1 = Only full-range ROC/AUC with no low-FPR detail,2 = Mentions “low FPR” qualitatively but no numbers/curves,3 = At least one operating point reported with FPR ≤1% or TPR@FPR shown once,4 = Provides bounded ROC/AUC or TPR values at FPR ≤0.5% and at least one operating point with thresholds,5 = Shows bounded ROC or reports TPR at very low FPRs (≤0.1%) with thresholds/CI,Figures/tables; look for ‘≤0.1% FPR’ or similar,
Inappropriate Measures,P3 -Q4,Alert Load (Operationalization),"Do they translate FPR into absolute false alerts (e.g., FP/day or FP/hour) using base rate/traffic volume?",Ordinal,1–5,N/A,N/A,1 = No alert-volume translation at all,2 = Gives only a hypothetical example without using the study’s own traffic volumes,3 = Computes FP/day (or FP/hour) at one threshold using a stated volume,"4 = Computes FP/day (or FP/hour) at multiple thresholds or volumes (e.g., sensitivity table)",5 = Presents FP/day (or FP/hour) at chosen thresholds and ties to analyst capacity/alert budget,Text/tables with traffic volumes and FP counts,
Inappropriate Measures,P3 -Q5,Per-Class Reporting,Are per-class Precision/Recall/F1 shown (not just aggregates)?,Ordinal,1–5,N/A,N/A,"1 = Aggregates only (macro/micro), no class-wise detail","2 = Per-class shown for a subset of classes or only one metric (e.g., recall only)","3 = Full per-class P/R/F1 for all classes or a confusion matrix, but minimal discussion",4 = Full per-class P/R/F1 and confusion matrix or clear narrative on weak classes,5 = Full per-class reporting with discussion,Tables per attack type; supplementary material,
Inappropriate Measures,P3 -Q6,Rare-Class Visibility,Do results highlight performance on rare/critical classes (not hidden by averages)?,Ordinal,1–5,N/A,N/A,1 = No mention; averages could hide failures,2 = Names rare/critical classes but no metrics provided,"3 = Provides metrics for rare classes (e.g., P/R/F1) or highlights at least one failure case","4 = Reports tail/worst-case stats (e.g., min-F1, count of classes with F1<0.2)","5 = Rare classes discussed; worst-case or tail metrics (min-F1, 10th-percentile) reported","Look for ‘rare class’, ‘long-tail’, ‘min-class F1’ discussion",
Inappropriate Measures,P3 -Q7,Class Balance Realism,Do they avoid artificial 50/50 balancing (or clearly justify and correct for it)?,Ordinal,1–5,N/A,N/A,1 = 50/50 split used without justification,2 = 50/50 used with textual justification but no corrective metrics,3 = Uses 50/50 for training but reports evaluation under realistic prevalence (or PR metrics),4 = Trains and evaluates with realistic ratios or explicitly reweights and reports PR/precision at base rate,5 = Realistic class ratios or justified adjustment + proper metrics,Dataset/preprocessing section,
Inappropriate Measures,P3 -Q8,Calibration/Thresholding,Are scores calibrated and thresholds selected on validation data (and re-validated under shifts)?,Ordinal,1–5,N/A,N/A,1 = No calibration/thresholding details,2 = Threshold selected on test set or ad-hoc; no calibration,3 = Threshold selected on validation set; no calibration metrics,4 = Uses calibration (Brier/ECE or reliability curves) or re-validates thresholds under distribution shift,"5 = Uses calibration (e.g., Brier/ECE) & thresholding best practices and validates under shift","Methods; look for Brier/ECE, reliability plots",
Inappropriate Measures,P3 -Q9,Multiple Views,"Do they present multiple complementary views (TPR/FPR, Precision/Recall, F1/MCC, ROC & PR)?",Ordinal,1–5,N/A,N/A,1 = Single metric view only,"2 = Two metrics of the same family (e.g., ROC + AUC)","3 = Includes both ROC and PR views, limited tabular metrics",4 = ROC + PR and tabular P/R/F1; possibly MCC,5 = Rich multi-view reporting (ROC & PR + P/R/F1 + MCC),Figures: both ROC & PR; tables include MCC,
Inappropriate Measures,P3 -Q10,Comparability,"Do they avoid mixing unfit metrics in ways that impede comparison (e.g., only accuracy vs others use PR)?",Ordinal,1–5,N/A,N/A,1 = Metrics chosen make cross-study comparison hard,"2 = Some comparable metrics but major gaps (e.g., missing PR in imbalanced setting)",3 = Uses common metrics but lacks key ones for imbalance or inconsistent thresholds,4 = Mostly fit-for-purpose metrics; minor inconsistencies documented,"5 = Uses comparable, fit-for-purpose metrics",Discussion on comparability,
Inappropriate Measures,P3 -Q11,Headline Claims,"Are headline claims (“99% accuracy”, “SOTA AUC”) tempered with precision/alert-load context?",Ordinal,1–5,N/A,N/A,1 = Hype without context,2 = Hedged language but no concrete context (no precision/FP-load),"3 = Adds at least one contextual metric (e.g., precision or FP/day) once",4 = Headlines consistently paired with precision and at least FPR,5 = Headlines include operational context (precision/FP-load),Abstract/conclusion claims,
Inappropriate Measures,P3 -Q12,Relevance to Deployment,"Do metrics align with deployment needs (low false alarms, manageable triage load)?",Ordinal,1–5,N/A,N/A,1 = Purely academic metrics with no tie to operations,2 = Vague mention of deployment needs; no numbers,3 = Provides an alert budget or analyst capacity but doesn’t tie to model thresholds,"4 = Ties thresholds to alert budget (e.g., FP/day ≤ team capacity)",5 = Clear tie to SOC workflows and budgets,Deployment/relevance section,
Inappropriate Measures,P3 -Q13,Class Imbalance Awareness,Does the paper explicitly state the class prevalence/base rate (attack:benign) used in evaluation?,Yes/No,1/0,0 = Not stated or unclear,"1 = Base rate is clearly stated (e.g., 1:1000) and used in analysis",N/A,N/A,N/A,N/A,N/A,"""Look for dataset description and prevalence; mention of “imbalance” with numbers""",This questions is very similar to P1-2 check if we need the 2 questions
Inappropriate Measures,P3 -Q14,PR Reporting,Are PR curves (or PR-AUC) reported for imbalanced settings?,Yes/No,1/0,0 = No PR reporting despite imbalance,1 = PR curve/PR-AUC present and discussed,N/A,N/A,N/A,N/A,N/A,Figures/tables (PR curves),
Inappropriate Measures,P3 -Q15,Operating Points,"Do they report concrete operating points aligned to alert budgets (e.g., “TPR at X FP/day”)?",Yes/No,1/0,"0 = Only aggregate metrics, no operating point",1 = Concrete operating points stated (TPR at FP/day),N/A,N/A,N/A,N/A,N/A,Results tables/‘deployment’ subsection,this question seems very related with a previous question about operating points. check if both are needed
Inappropriate Measures,P3 -Q16,Aggregate Metric Use,"If macro/micro/weighted averages are used, are their limitations discussed (risk of hiding failures)?",Yes/No,1/0,0 = No caveats about aggregation,1 = Limitations of averaging are noted,N/A,N/A,N/A,N/A,N/A,Metric discussion around macro/micro averaging,
Inappropriate Measures,P3 -Q17,Low-FPR Evidence,Do they show performance at *very* low FPR (≤0.1%) typical of IDS operations?,Yes/No,1/0,0 = No,1 = Yes,N/A,N/A,N/A,N/A,N/A,Figures/tables listing ≤0.1% FPR rows/axes; Confusion matrices,
Inappropriate Measures,P3 -Q18,Cost Asymmetry,Do they acknowledge asymmetric costs of FP vs FN in security?,Yes/No,1/0,0 = No cost asymmetry discussion,1 = Discusses operational costs/asymmetry,N/A,N/A,N/A,N/A,N/A,Problem setup/metrics rationale,
Inappropriate Measures,P3 -Q19,Domain Baselines,"Do they contextualize against relevant domain baselines (e.g., signature IDS) with target precision/FPR?",Yes/No,1/0,0 = No domain baseline context,1 = Baselines + target deployability thresholds discussed,N/A,N/A,N/A,N/A,N/A,Baselines section; deployability criteria,
Inappropriate Baseline,P3 -Q20,Cross-Study Comparability,"Are baselines and results comparable to prior IDS studies, supporting cross-validation with literature?",Ordinal,1–5,N/A,N/A,Unique setup; not comparable,Some overlap with prior work,Similar design but inconsistent,Largely consistent,Fully comparable setup using accepted benchmarks,Methods / Literature review,